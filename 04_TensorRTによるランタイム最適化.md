# TensorRTによるランタイム最適化
### TensorRTとは
NVIDIA社が提供する、ディープラーニングの推論において低遅延や高いスループットを提供するソフトウェア開発キットです。  
CUDA-X™のライブラリや開発ツール、テクノロジを利用して、推論の最適化を行い、PyTorchやTensorFlow、ONNXとの統合により高速かつ高性能な推論が実現可能です。  
さらに、アプリケーションに固有なSDKとも統合されており、知的なビデオの分析、レコメンダーシステムなどの展開に統一されたパスを提供しています。  
そして、量子化対応トレーニングとトレーニング後の量子化を使用したINT8とビデオストリーミングや音声認識、自然言語処理などのディープラーニング推論アプリケーションの実稼働展開のためのFP16の最適化を行うことができます。

![architecture](./image/4-1TensorRT_Architecture.png)

### TensorRTを使用しない場合と比べたときのメリット
メリットとしては、推論時間を短縮できる事、メモリ使用量を減少させられる事、アプリケーションの遅延を最小限に抑えられる事、Tensorflow、 PyTorchなどの主要フレームワークのサポートができる事などが挙げられます。  
また、データセンターのほかに組み込み環境や自動車環境にも展開可能である事や推論の展開に統一されたパスを提供できる事も強みに挙げることができます。

### 推論エンジンの生成
ランタイム最適化をする上で推論エンジンの生成を行っていて、推論の高速化をするために、学習済みネットワークの最適化を行っています。  
最適化処理の内容としては

1. 重みと活性化関数の精度キャリブレーション<
モデルの計算量を減らすことで、精度を維持したまま、スループットを最大化する
2. レイヤーとテンソルの融合
レイヤー同士またはテンソル同士を融合することで、レイヤー数を減らし、ネットワークを最適化する
3. カーネルの自動チューニング
各GPUプラットフォームにおいて最適化されたデータレイヤーとアルゴリズムを選択する
4. 動的テンソルメモリ
メモリ使用量の最小化とメモリの再利用を行う
5. マルチストリーム実行
複数のストリーム（GPU上で実行する一連の操作）で推論を並列的に実行する
6. 時間融合
動的に生み出されたカーネルで時間とともにニューラルネットワークを再構築する
  
などがあリます。

### 推論実行
そして、最後に推論実行をするにあたり変換と実行→TensorRTエンジンをファイルから逆シリアル化し、デプロイを行います。  

手順（Python API）
1. ファイルの逆シリアル化
   ```
   runtime = trt.Runtime(logger)
   engine = runtime.deserialize_cuda_engine(serialized_engine)
   ```
2. 推論の実行
   ```
   context = engine.create_execution_context()
   ```
   [入力と出力のTensorRTバッファーの配列内の正しい位置を見つけるためにGPUポインターのリストで指定]
   ```
   input_idx = engine[input_name]
   output_idx = engine[output_name]
   ```
   [GPUポインターのリストを作成]
   ```
   buffers = [None] * 2 # Assuming 1 input and 1 output
   buffers[input_idx] = input_ptr
   buffers[output_idx] = output_ptr
   ```
   [推論の実行]
   ```
   context.execute_async_v2(buffers, stream_ptr)
   ```